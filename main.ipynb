{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017c097f",
   "metadata": {},
   "source": [
    "Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Import local modules\n",
    "from utils.preprocess import (\n",
    "    normalize_case, clean_punct_and_numbers, tokenize,\n",
    "    remove_stopwords, lemmatize, stem_tokens, identity,\n",
    "    preprocess_text, get_vectorizer\n",
    ")\n",
    "from llm.gpt_zero_shot_classifier import classify_zero_shot, classify_zero_shot_batch\n",
    "from llm.mistral_classifier import classify_mistral, classify_mistral_batch\n",
    "\n",
    "DATA_PATH = os.path.join(\"data\", \"klikšķēsma.txt\")\n",
    "MODELS_DIR = \"models\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584da7d6",
   "metadata": {},
   "source": [
    "Load and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"title\", \"label\"],\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "assert len(df) == 4930, f\"Expected 4930 samples, got {len(df)}\"\n",
    "\n",
    "# Class distribution analysis\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "print(f\"\\nClass percentages:\")\n",
    "print((df['label'].value_counts(normalize=True).sort_index() * 100).round(1))\n",
    "\n",
    "# Validate data\n",
    "expected_counts = {1: 3306, 2: 440, 3: 1184}\n",
    "actual_counts = df['label'].value_counts().to_dict()\n",
    "print(\"\\nData validation:\")\n",
    "for label, expected in expected_counts.items():\n",
    "    actual = actual_counts.get(label, 0)\n",
    "    status = \"✓\" if expected == actual else \"✗\"\n",
    "    print(f\"Class {label}: Expected {expected}, Got {actual} {status}\")\n",
    "\n",
    "# Show sample headlines\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample headlines by class:\")\n",
    "print(\"=\"*80)\n",
    "for label in [1, 2, 3]:\n",
    "    class_name = {1: 'Nav klikšķēsma', 2: 'Daļēja klikšķēsma', 3: 'Ir klikšķēsma'}[label]\n",
    "    print(f\"\\nClass {label} ({class_name}):\")\n",
    "    samples = df[df['label'] == label].sample(3, random_state=42)\n",
    "    for idx, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        print(f\"  {idx}. {row['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39f8af",
   "metadata": {},
   "source": [
    "Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc82404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text features\n",
    "df['word_count'] = df['title'].str.split().str.len()\n",
    "df['char_count'] = df['title'].str.len()\n",
    "df['has_number'] = df['title'].str.contains(r'\\d+', regex=True)\n",
    "df['has_exclamation'] = df['title'].str.contains('!')\n",
    "df['has_question'] = df['title'].str.contains('\\?')\n",
    "\n",
    "# Statistical analysis by class\n",
    "print(\"Statistical analysis by class:\")\n",
    "print(\"=\"*60)\n",
    "stats = df.groupby('label').agg({\n",
    "    'word_count': ['mean', 'std'],\n",
    "    'char_count': ['mean', 'std'],\n",
    "    'has_number': 'mean',\n",
    "    'has_exclamation': 'mean',\n",
    "    'has_question': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "for label in [1, 2, 3]:\n",
    "    class_name = {1: 'Nav klikšķēsma', 2: 'Daļēja klikšķēsma', 3: 'Ir klikšķēsma'}[label]\n",
    "    print(f\"\\nClass {label} ({class_name}):\")\n",
    "    print(f\"  Average word count: {stats.loc[label, ('word_count', 'mean')]:.1f} (±{stats.loc[label, ('word_count', 'std')]:.1f})\")\n",
    "    print(f\"  Average char count: {stats.loc[label, ('char_count', 'mean')]:.1f} (±{stats.loc[label, ('char_count', 'std')]:.1f})\")\n",
    "    print(f\"  Contains numbers: {stats.loc[label, ('has_number', 'mean')]*100:.1f}%\")\n",
    "    print(f\"  Contains exclamation: {stats.loc[label, ('has_exclamation', 'mean')]*100:.1f}%\")\n",
    "    print(f\"  Contains question: {stats.loc[label, ('has_question', 'mean')]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e39f1f",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Clickbait Data Analysis', fontsize=16)\n",
    "\n",
    "# Word count distribution\n",
    "for label in [1, 2, 3]:\n",
    "    data = df[df['label'] == label]['word_count']\n",
    "    axes[0, 0].hist(data, alpha=0.6, bins=20, label=f'Class {label}', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Word Count')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Word Count Distribution by Class')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average word count bar chart\n",
    "avg_words = df.groupby('label')['word_count'].mean()\n",
    "bars = axes[0, 1].bar(['1-Nav', '2-Daļēja', '3-Ir'], avg_words.values, \n",
    "                      color=['green', 'orange', 'red'], alpha=0.7)\n",
    "axes[0, 1].set_ylabel('Average Word Count')\n",
    "axes[0, 1].set_title('Average Word Count by Class')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Feature presence comparison\n",
    "features = ['has_number', 'has_exclamation', 'has_question']\n",
    "feature_means = df.groupby('label')[features].mean()\n",
    "x = np.arange(len(feature_means.index))\n",
    "width = 0.25\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    offset = (i - 1) * width\n",
    "    feature_label = {'has_number': 'Numbers', \n",
    "                    'has_exclamation': 'Exclamation', \n",
    "                    'has_question': 'Question'}[feature]\n",
    "    axes[1, 0].bar(x + offset, feature_means[feature], width, \n",
    "                   label=feature_label, alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_xlabel('Class')\n",
    "axes[1, 0].set_ylabel('Proportion')\n",
    "axes[1, 0].set_title('Feature Presence by Class')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(['1-Nav', '2-Daļēja', '3-Ir'])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Class distribution pie chart\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "colors = ['#90EE90', '#FFD700', '#FF6B6B']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "axes[1, 1].pie(class_counts.values, labels=['1-Nav\\n(67.1%)', '2-Daļēja\\n(8.9%)', '3-Ir\\n(24.0%)'], \n",
    "               autopct='%d', startangle=90, colors=colors, explode=explode)\n",
    "axes[1, 1].set_title('Class Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8373f3",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"title\"],\n",
    "    df[\"label\"],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "assert len(X_test) == 1479, f\"Expected 1479 test samples, got {len(X_test)}\"\n",
    "\n",
    "# Preprocess texts\n",
    "print(\"\\nPreprocessing texts...\")\n",
    "X_train_clean = X_train.apply(\n",
    "    lambda t: preprocess_text(t, casefold=False, morpho_method=\"lemmatize\")\n",
    ")\n",
    "X_test_clean = X_test.apply(\n",
    "    lambda t: preprocess_text(t, casefold=False, morpho_method=\"lemmatize\")\n",
    ")\n",
    "\n",
    "# Vectorize with TF-IDF\n",
    "print(\"\\nVectorizing with TF-IDF...\")\n",
    "print(\"Parameters: max_features=1000, ngram_range=(1,2), min_df=2, sublinear_tf=True\")\n",
    "vectorizer = get_vectorizer(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train_clean)\n",
    "X_test_vec = vectorizer.transform(X_test_clean)\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shapes:\")\n",
    "print(f\"Train: {X_train_vec.shape}\")\n",
    "print(f\"Test: {X_test_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e0a82",
   "metadata": {},
   "source": [
    "Train Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df724af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"logistic_regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"naive_bayes\": MultinomialNB(),\n",
    "    \"svm\": LinearSVC(random_state=42, max_iter=2000),\n",
    "    \"random_forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = {}\n",
    "print(\"\\nTraining Classical Models...\")\n",
    "print(\"=\"*80)\n",
    "header = f\"{'Model':<20} {'Acc':<6} {'Prec':<6} {'Rec':<6} {'F1':<6} {'ROC-AUC':<8}\"\n",
    "print(header)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train model\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    # Calculate ROC-AUC if possible\n",
    "    roc_auc = None\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(X_test_vec)\n",
    "        roc_auc = roc_auc_score(y_test, y_score, multi_class=\"ovo\", average=\"macro\")\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        y_score = clf.decision_function(X_test_vec)\n",
    "        if len(y_score.shape) > 1:\n",
    "            y_score = np.exp(y_score) / np.sum(np.exp(y_score), axis=1, keepdims=True)\n",
    "            roc_auc = roc_auc_score(y_test, y_score, multi_class=\"ovo\", average=\"macro\")\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': y_pred,\n",
    "        'model': clf\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    roc_str = f\"{roc_auc:.3f}\" if roc_auc else \"N/A\"\n",
    "    print(f\"{name:<20} {acc:<6.3f} {prec:<6.3f} {rec:<6.3f} {f1:<6.3f} {roc_str:<8}\")\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(clf, os.path.join(MODELS_DIR, f\"{name}.joblib\"))\n",
    "\n",
    "# Save vectorizer\n",
    "joblib.dump(vectorizer, os.path.join(MODELS_DIR, \"tfidf_vectorizer.joblib\"))\n",
    "print(\"\\nModels and vectorizer saved to:\", MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02041cef",
   "metadata": {},
   "source": [
    "Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59afa673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text features\n",
    "df['word_count'] = df['title'].str.split().str.len()\n",
    "df['char_count'] = df['title'].str.len()\n",
    "df['has_number'] = df['title'].str.contains(r'\\d+', regex=True)\n",
    "df['has_exclamation'] = df['title'].str.contains('!')\n",
    "df['has_question'] = df['title'].str.contains('\\?')\n",
    "\n",
    "# Statistical analysis by class\n",
    "print(\"Statistical analysis by class:\")\n",
    "print(\"=\"*60)\n",
    "stats = df.groupby('label').agg({\n",
    "    'word_count': ['mean', 'std'],\n",
    "    'char_count': ['mean', 'std'],\n",
    "    'has_number': 'mean',\n",
    "    'has_exclamation': 'mean',\n",
    "    'has_question': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "for label in [1, 2, 3]:\n",
    "    class_name = {1: 'Nav klikšķēsma', 2: 'Daļēja klikšķēsma', 3: 'Ir klikšķēsma'}[label]\n",
    "    print(f\"\\nClass {label} ({class_name}):\")\n",
    "    print(f\"  Average word count: {stats.loc[label, ('word_count', 'mean')]:.1f} (±{stats.loc[label, ('word_count', 'std')]:.1f})\")\n",
    "    print(f\"  Average char count: {stats.loc[label, ('char_count', 'mean')]:.1f} (±{stats.loc[label, ('char_count', 'std')]:.1f})\")\n",
    "    print(f\"  Contains numbers: {stats.loc[label, ('has_number', 'mean')]*100:.1f}%\")\n",
    "    print(f\"  Contains exclamation: {stats.loc[label, ('has_exclamation', 'mean')]*100:.1f}%\")\n",
    "    print(f\"  Contains question: {stats.loc[label, ('has_question', 'mean')]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a20e88",
   "metadata": {},
   "source": [
    "Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b04cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# F1 scores comparison\n",
    "models = list(results.keys())\n",
    "f1_scores = [results[m]['f1'] for m in models]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "\n",
    "bars = ax1.bar(range(len(models)), f1_scores, color=colors, alpha=0.8)\n",
    "ax1.set_xticks(range(len(models)))\n",
    "ax1.set_xticklabels([m.replace('_', ' ').title() for m in models], rotation=45, ha='right')\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_title('Classical Models F1 Score Comparison')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# All metrics comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [results[m][metric] for m in models]\n",
    "    ax2.bar(x + i*width - 1.5*width, values, width, label=metric.capitalize(), alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('All Metrics Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([m.replace('_', ' ').title() for m in models], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f3924",
   "metadata": {},
   "source": [
    "Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c68509",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best Classical Model: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_predictions, \n",
    "                          target_names=['1-Nav', '2-Daļēja', '3-Ir']))\n",
    "\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['1-Nav', '2-Daļēja', '3-Ir'],\n",
    "            yticklabels=['1-Nav', '2-Daļēja', '3-Ir'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name.replace(\"_\", \" \").title()}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nError Analysis:\")\n",
    "errors = y_test != best_predictions\n",
    "error_count = errors.sum()\n",
    "print(f\"Total errors: {error_count} out of {len(y_test)} ({error_count/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d2217",
   "metadata": {},
   "source": [
    "Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# F1 scores comparison\n",
    "models = list(results.keys())\n",
    "f1_scores = [results[m]['f1'] for m in models]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "\n",
    "bars = ax1.bar(range(len(models)), f1_scores, color=colors, alpha=0.8)\n",
    "ax1.set_xticks(range(len(models)))\n",
    "ax1.set_xticklabels([m.replace('_', ' ').title() for m in models], rotation=45, ha='right')\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_title('Classical Models F1 Score Comparison')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# All metrics comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [results[m][metric] for m in models]\n",
    "    ax2.bar(x + i*width - 1.5*width, values, width, label=metric.capitalize(), alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('All Metrics Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([m.replace('_', ' ').title() for m in models], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbf9c8",
   "metadata": {},
   "source": [
    "Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043bf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best Classical Model: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_predictions, \n",
    "                          target_names=['1-Nav', '2-Daļēja', '3-Ir']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['1-Nav', '2-Daļēja', '3-Ir'],\n",
    "            yticklabels=['1-Nav', '2-Daļēja', '3-Ir'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name.replace(\"_\", \" \").title()}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "print(\"\\nError Analysis:\")\n",
    "errors = y_test != best_predictions\n",
    "error_count = errors.sum()\n",
    "print(f\"Total errors: {error_count} out of {len(y_test)} ({error_count/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8ac36",
   "metadata": {},
   "source": [
    "LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check OpenAI API availability\n",
    "import openai\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"OpenAI API key found.\")\n",
    "else:\n",
    "    print(\"Warning: OPENAI_API_KEY not set. GPT models will be skipped.\")\n",
    "    print(\"To set: export OPENAI_API_KEY='your-key-here'\")\n",
    "\n",
    "# Demo mode for LLM testing\n",
    "DEMO_MODE = True\n",
    "if DEMO_MODE:\n",
    "    print(\"\\n*** DEMO MODE: Using only 50 samples for LLM evaluation ***\")\n",
    "    test_headlines = X_test.iloc[:50].tolist()\n",
    "    y_test_llm = y_test.iloc[:50]\n",
    "else:\n",
    "    test_headlines = X_test.tolist()\n",
    "    y_test_llm = y_test\n",
    "\n",
    "print(f\"\\nEvaluating LLM Models on {len(test_headlines)} headlines...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "llm_results = {}\n",
    "\n",
    "# Test GPT-3.5 if API key is available\n",
    "if api_key and DEMO_MODE:\n",
    "    try:\n",
    "        print(\"Testing GPT-3.5 Turbo...\")\n",
    "        preds_gpt35 = classify_zero_shot_batch(test_headlines, model=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        # Filter valid predictions\n",
    "        valid_indices = [i for i, p in enumerate(preds_gpt35) if p is not None]\n",
    "        if len(valid_indices) < len(preds_gpt35):\n",
    "            print(f\"Warning: {len(preds_gpt35) - len(valid_indices)} failed classifications\")\n",
    "        \n",
    "        y_test_valid = y_test_llm.iloc[valid_indices].values\n",
    "        preds_valid = [preds_gpt35[i] for i in valid_indices]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test_valid, preds_valid)\n",
    "        prec = precision_score(y_test_valid, preds_valid, average=\"macro\")\n",
    "        rec = recall_score(y_test_valid, preds_valid, average=\"macro\")\n",
    "        f1 = f1_score(y_test_valid, preds_valid, average=\"macro\")\n",
    "        \n",
    "        llm_results['GPT-3.5'] = {\n",
    "            'accuracy': acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1\n",
    "        }\n",
    "        \n",
    "        print(f\"GPT-3.5: Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPT-3.5 error: {e}\")\n",
    "\n",
    "if not DEMO_MODE:\n",
    "    print(\"\\nFor full evaluation with all LLM models, run train_classical_models.py and evaluate_models.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac9d6e",
   "metadata": {},
   "source": [
    "Final Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_f1_scores = {name: res['f1'] for name, res in results.items()}\n",
    "\n",
    "# Add LLM results\n",
    "if not DEMO_MODE:\n",
    "    # Full evaluation results (from paper)\n",
    "    all_f1_scores.update({\n",
    "        'GPT-3.5': 0.761,\n",
    "        'GPT-4 Turbo': 0.817,\n",
    "        'Mistral 7B': 0.773\n",
    "    })\n",
    "else:\n",
    "    # Demo mode results\n",
    "    all_f1_scores.update({name: res['f1'] for name, res in llm_results.items()})\n",
    "\n",
    "# Create final comparison chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "models = list(all_f1_scores.keys())\n",
    "scores = list(all_f1_scores.values())\n",
    "colors = ['skyblue'] * 5 + ['lightcoral'] * (len(models) - 5)\n",
    "\n",
    "bars = plt.bar(range(len(models)), scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = scores.index(max(scores))\n",
    "bars[best_idx].set_color('gold')\n",
    "bars[best_idx].set_edgecolor('darkgoldenrod')\n",
    "bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.xticks(range(len(models)), [m.replace('_', ' ').title() for m in models], rotation=45, ha='right')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('All Models Comparison: Classical ML vs LLM', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='skyblue', alpha=0.8, label='Classical Models'),\n",
    "    Patch(facecolor='lightcoral', alpha=0.8, label='LLM Models'),\n",
    "    Patch(facecolor='gold', alpha=0.8, label='Best Model')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "best_model = max(all_f1_scores.items(), key=lambda x: x[1])\n",
    "print(f\"Best model overall: {best_model[0]} with F1-score: {best_model[1]:.3f}\")\n",
    "print(f\"Best classical model: {best_model_name} with F1-score: {results[best_model_name]['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca26c6",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd88442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_headline(headline, model_name=\"svm\"):\n",
    "    \"\"\"Predict if a headline is clickbait using specified model.\"\"\"\n",
    "    # Preprocess text\n",
    "    clean = preprocess_text(headline, casefold=False, morpho_method=\"lemmatize\")\n",
    "    vec = vectorizer.transform([clean])\n",
    "    \n",
    "    # Load model\n",
    "    if model_name in results:\n",
    "        model = results[model_name]['model']\n",
    "    else:\n",
    "        model_path = os.path.join(MODELS_DIR, f\"{model_name}.joblib\")\n",
    "        model = joblib.load(model_path)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(vec)[0]\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    proba = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(vec)[0]\n",
    "    \n",
    "    # Display results\n",
    "    class_names = {1: 'Nav klikšķēsma', 2: 'Daļēja klikšķēsma', 3: 'Ir klikšķēsma'}\n",
    "    \n",
    "    print(f\"\\nHeadline: '{headline}'\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Prediction: Class {pred} - {class_names[pred]}\")\n",
    "    if proba is not None:\n",
    "        print(f\"Confidence: [Class 1: {proba[0]:.2%}, Class 2: {proba[1]:.2%}, Class 3: {proba[2]:.2%}]\")\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"Prezidents paziņo par jauniem ierobežojumiem\",\n",
    "    \"6 padomi, kā ietaupīt naudu\",\n",
    "    \"Šī viena metode mainīs tavu rītu!\"\n",
    "]\n",
    "\n",
    "print(\"\\nInteractive Prediction Demo:\")\n",
    "print(\"=\"*80)\n",
    "for headline in test_examples:\n",
    "    predict_headline(headline, model_name=best_model_name)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fe35d",
   "metadata": {},
   "source": [
    "Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Assuming df, X_train, X_test, results, llm_results, and best_model are defined earlier\n",
    "\n",
    "# Build dynamic summary\n",
    "summary = {\n",
    "    'experiment_date': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_size': len(df),\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'class_distribution': df['label'].value_counts().to_dict()\n",
    "    },\n",
    "    'preprocessing_config': preprocessing_config,\n",
    "    'vectorizer_config': vectorizer_config,\n",
    "    'classical_models': {\n",
    "        name: {\n",
    "            'accuracy': res['accuracy'],\n",
    "            'precision': res['precision'],\n",
    "            'recall': res['recall'],\n",
    "            'f1': res['f1'],\n",
    "            'roc_auc': res['roc_auc']\n",
    "        }\n",
    "        for name, res in results.items()\n",
    "    },\n",
    "    'llm_models': llm_results,  # always use dynamic LLM results\n",
    "    'best_model': {\n",
    "        'name': best_model[0],\n",
    "        'f1_score': best_model[1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_file = 'complete_experiment_results.json'\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Complete results saved to '{results_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
